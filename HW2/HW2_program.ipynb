{
 "cells": [
  {
   "source": [],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Import libraries that you might require.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import operator\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "\n",
    "breast = load_breast_cancer()\n",
    "\n",
    "X = breast['data']\n",
    "y = breast['target']\n",
    "\n",
    "np.random.seed(100)\n",
    "p = np.random.permutation(len(X))\n",
    "X, y = X[p], y[p]\n",
    "\n",
    "X_train, y_train = X[:400], y[:400]\n",
    "X_val, y_val = X[400:500], y[400:500]\n",
    "X_test, y_test = X[500:], y[500:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distanceFunc(metric_type, vec1, vec2):\n",
    "    \"\"\"\n",
    "    Computes the distance between two d-dimension vectors. \n",
    "    \n",
    "    Please DO NOT use Numpy's norm function when implementing this function. \n",
    "    \n",
    "    Args:\n",
    "        metric_type (str): Metric: L1, L2, or L-inf\n",
    "        vec1 ((d,) np.ndarray): d-dim vector\n",
    "        vec2 ((d,)) np.ndarray): d-dim vector\n",
    "    \n",
    "    Returns:\n",
    "        distance (float): distance between the two vectors\n",
    "    \"\"\"\n",
    "    diff = vec1 - vec2\n",
    "    distance = 0\n",
    "    if metric_type == \"L1\":\n",
    "        diff_abs = np.abs(diff)\n",
    "        distance = np.sum(diff_abs) #complete\n",
    "\n",
    "    if metric_type == \"L2\":\n",
    "        diff_square = np.square(diff)\n",
    "        distance = np.sqrt(np.sum(diff_square)) #complete\n",
    "        \n",
    "    if metric_type == \"L-inf\":\n",
    "        diff_abs = np.abs(diff)\n",
    "        distance = np.max(diff_abs) #complete\n",
    "\n",
    "    return distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeDistancesNeighbors(K, metric_type, X_train, y_train, sample):\n",
    "    \"\"\"\n",
    "    Compute the distances between every datapoint in the train_data and the \n",
    "    given sample. Then, find the k-nearest neighbors.\n",
    "    \n",
    "    Return a numpy array of the label of the k-nearest neighbors.\n",
    "    \n",
    "    Args:\n",
    "        K (int): K-value\n",
    "        metric_type (str): metric type\n",
    "        X_train ((n,p) np.ndarray): Training data with n samples and p features\n",
    "        y_train : Training labels\n",
    "        sample ((p,) np.ndarray): Single sample whose distance is to computed with every entry in the dataset\n",
    "        \n",
    "    Returns:\n",
    "        neighbors (list): K-nearest neighbors' labels\n",
    "    \"\"\"\n",
    "\n",
    "    # You will also call the function \"distanceFunc\" here\n",
    "    # Complete this function\n",
    "\n",
    "    # neighbors = []\n",
    "    # for i,item in enumerate(X_train):\n",
    "    #     for j,item in enumerate(X_train[i,:]):\n",
    "    #         for i_k,item in enumerate(X_train):\n",
    "    #             for j_k,item in enumerate(X_train[i,:]):\n",
    "    #                 vec1 = X_train[i,j]\n",
    "    #                 vec2 = X_train[i_k,j_k]\n",
    "    #                 neighbors.append(distanceFunc(\"L1\", vec1, vec2))\n",
    "    # return neighbors\n",
    "\n",
    "    neighbors = []\n",
    "\n",
    "    #output = np.zeros((X_test.shape[0],1))\n",
    "    # for i in range(X_test.shape[0]):\n",
    "    dis = [] \n",
    "    for i in range(X_train.shape[0]):\n",
    "        #dis.append(np.linalg.norm(X_test[i,:]-self.x[j,:]))\n",
    "        dis.append(distanceFunc(metric_type,sample,X_train[i,:]))\n",
    "        \n",
    "\n",
    "    labels = []\n",
    "    index=sorted(range(len(dis)), key=dis.__getitem__)\n",
    "    for j in range(K):\n",
    "        labels.append(y_train[index[j]])\n",
    "    neighbors = labels\n",
    "\n",
    "    return neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Majority(neighbors):\n",
    "    \"\"\"\n",
    "    Performs majority voting and returns the predicted value for the test sample.\n",
    "    \n",
    "    Since we're performing binary classification the possible values are [0,1].\n",
    "    \n",
    "    Args:\n",
    "        neighbors (list): K-nearest neighbors' labels\n",
    "        \n",
    "    Returns:\n",
    "        predicted_value (int): predicted label for the given sample\n",
    "    \"\"\"\n",
    "    \n",
    "    # Performs majority voting\n",
    "    # Complete this function\n",
    "\n",
    "    counts = []\n",
    "    for neighbor in neighbors:\n",
    "        counts.append(neighbors.count(neighbor))\n",
    "    predicted_value = neighbors[np.argmax(counts)]\n",
    "    return predicted_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def KNN(K, metric_type, X_train, y_train, X_val):\n",
    "    \"\"\"\n",
    "    Returns the predicted values for the entire validation or test set.\n",
    "    \n",
    "    Please DO NOT use Scikit's KNN model when implementing this function. \n",
    "\n",
    "    Args:\n",
    "        K (int): K-value\n",
    "        metric_type (str): metric type\n",
    "        X_train ((n,p) np.ndarray): Training data with n samples and p features\n",
    "        y_train : Training labels\n",
    "        X_val ((n, p) np.ndarray): Validation or test data\n",
    "        \n",
    "    Returns:\n",
    "        predicted_values (list): output for every entry in validation/test dataset \n",
    "    \"\"\"\n",
    "    \n",
    "    # Complete this function\n",
    "    # Loop through the val_data or the test_data (as required)\n",
    "    # and compute the output for every entry in that dataset  \n",
    "    # You will also call the function \"Majority\" here\n",
    "\n",
    "    predictions = []\n",
    "    for i in range(X_val.shape[0]):\n",
    "        #neighbors = []\n",
    "        sample = X_val[i,:]\n",
    "        neighbors = computeDistancesNeighbors(K, metric_type, X_train, y_train, sample)\n",
    "\n",
    "        predicted_value = Majority(neighbors)\n",
    "        #print(predicted_value)\n",
    "        predictions.append(predicted_value)\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation(predicted_values, actual_values):\n",
    "    \"\"\"\n",
    "    Computes the accuracy of the given datapoints.\n",
    "    \n",
    "    Args:\n",
    "        predicted_values ((n,) np.ndarray): Predicted values for n samples\n",
    "        actual_values ((n,) np.ndarray): Actual values for n samples\n",
    "    \n",
    "    Returns:\n",
    "        accuracy (float): accuracy\n",
    "    \"\"\"\n",
    "    acc = accuracy_score(predicted_values, actual_values)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    \"\"\"\n",
    "    Calls the above functions in order to implement the KNN algorithm.\n",
    "    \n",
    "    Test over the following range K = 3,5,7 and all three metrics. \n",
    "    In total you will have nine combinations to try.\n",
    "    \n",
    "    PRINTS out the accuracies for the nine combinations on the validation set,\n",
    "    and the accuracy on the test set for the selected K value and appropriate norm.\n",
    "    \n",
    "    REMEMBER: You have to report these values by populating the Table 1.\n",
    "    \"\"\"\n",
    "    \n",
    "    ## Complete this function\n",
    "    \n",
    "    K = [3,5,7]\n",
    "    norm = [\"L1\", \"L2\", \"L-inf\"]\n",
    "    \n",
    "    print(\"<<<<VALIDATION DATA PREDICTIONS>>>>\")\n",
    "    \n",
    "    ## Complete\n",
    "    for i in [3,5,7]:\n",
    "        print('i is:'+ str(i))\n",
    "        predictions = KNN(i, \"L1\", X_train, y_train, X_val)\n",
    "        acc = evaluation(predictions,y_val)\n",
    "        print('acc of KNN L1 is %0.3f' %acc)\n",
    "        \n",
    "        predictions = KNN(i, \"L2\", X_train, y_train, X_val)\n",
    "        acc = evaluation(predictions,y_val)\n",
    "        print('acc of KNN L2 is %0.3f' %acc)\n",
    "\n",
    "        predictions = KNN(i, \"L-inf\", X_train, y_train, X_val)\n",
    "        acc = evaluation(predictions,y_val)\n",
    "        print('acc of KNN L-inf is %0.3f' %acc)\n",
    "\n",
    "    print(\"<<<<TEST DATA PREDICTIONS>>>>\")\n",
    "    \n",
    "    ## Complete\n",
    "\n",
    "    for i in [3,5,7]:\n",
    "        print('i is:'+ str(i))\n",
    "        predictions = KNN(i, \"L1\", X_train, y_train, X_test)\n",
    "        acc = evaluation(predictions,y_test)\n",
    "        print('acc of KNN L1 is %0.3f' %acc)\n",
    "        \n",
    "        predictions = KNN(i, \"L2\", X_train, y_train, X_test)\n",
    "        acc = evaluation(predictions,y_test)\n",
    "        print('acc of KNN L2 is %0.3f' %acc)\n",
    "\n",
    "        predictions = KNN(i, \"L-inf\", X_train, y_train, X_test)\n",
    "        acc = evaluation(predictions,y_test)\n",
    "        print('acc of KNN L-inf is %0.3f' %acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "<<<<VALIDATION DATA PREDICTIONS>>>>\ni is:3\nacc of KNN L1 is 0.940\nacc of KNN L2 is 0.950\nacc of KNN L-inf is 0.940\ni is:5\nacc of KNN L1 is 0.940\nacc of KNN L2 is 0.930\nacc of KNN L-inf is 0.940\ni is:7\nacc of KNN L1 is 0.930\nacc of KNN L2 is 0.920\nacc of KNN L-inf is 0.930\n<<<<TEST DATA PREDICTIONS>>>>\ni is:3\nacc of KNN L1 is 0.884\nacc of KNN L2 is 0.884\nacc of KNN L-inf is 0.899\ni is:5\nacc of KNN L1 is 0.913\nacc of KNN L2 is 0.899\nacc of KNN L-inf is 0.899\ni is:7\nacc of KNN L1 is 0.899\nacc of KNN L2 is 0.913\nacc of KNN L-inf is 0.899\n"
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "(100, 30)"
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "source": [
    "X_val.shape[:]\n"
   ]
  },
  {
   "source": [
    "https://www.cnblogs.com/lyuzt/p/10471617.html\n",
    "https://www./beikew/p/10246883.html\n",
    "https://numpy.org/doc/stable/reference/generated/numpy.linalg.norm.html\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "Python 3.7.7 64-bit",
   "display_name": "Python 3.7.7 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "0600588c3b5f4418cbe7b5ebc6825b479f3bc010269d8b60d75058cdd010adfe"
    }
   }
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}