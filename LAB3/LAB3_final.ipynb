{
 "cells": [
  {
   "source": [
    "# CS405 Machine Learning: Lab 2 Preliminary\n",
    "### Name: 车凯威\n",
    "### ID: 12032207"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "__Objectives__：Text mining (deriving information from text) is a wide field which has\n",
    "gained popularity with the huge text data being generated. Automation of a\n",
    "number of applications like sentiment analysis, document classification, topic classification, text summarization, machine translation, etc., has been\n",
    "done using machine learning models. In this lab, you are required to write\n",
    "your spam filter by using naïve Bayes method. This time you should not\n",
    "use 3\n",
    "rd party libraries including scikit-learn."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Instruction:\n",
    "Spam filtering is a beginner’s example of document classification task\n",
    "which involves classifying an email as spam or non-spam (a.k.a. ham) mail. Email dataset will be provided. We will walk through the following steps\n",
    "to build this application:  \n",
    "1) Preparing the text data  \n",
    "2) Creating word dictionary  \n",
    "3) Feature extraction process  \n",
    "4) Training the classifier  \n",
    "5) Checking the results on test set  "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 1 Preparing the text data:\n",
    "The data-set used here, is split into a training set and a test set containing\n",
    "702 mails and 260 mails respectively, divided equally between spam and\n",
    "ham mails. You will easily recognize spam mails as it contains *spmsg*\n",
    "in its filename.\n",
    "\n",
    "In any text mining problem, text cleaning is the first step where we\n",
    "remove those words from the document which may not contribute to the\n",
    "information we want to extract. Emails may contain a lot of undesirable\n",
    "characters like punctuation marks, stop words, digits, etc which may not\n",
    "be helpful in detecting the spam email. The emails in Ling-spam corpus\n",
    "have been already preprocessed in the following ways:  \n",
    "\n",
    "a) Removal of stop words – Stop words like “and”, “the”, “of”, etc are\n",
    "very common in all English sentences and are not very meaningful in\n",
    "deciding spam or legitimate status, so these words have been removed\n",
    "from the emails.   \n",
    "\n",
    "b) Lemmatization – It is the process of grouping together the different\n",
    "inflected forms of a word so they can be analysed as a single item. For\n",
    "example, “include”, “includes,” and “included” would all be\n",
    "represented as “include”. The context of the sentence is also preserved\n",
    "in lemmatization as opposed to stemming (another buzz word in text\n",
    "mining which does not consider meaning of the sentence)  \n",
    "\n",
    "We still need to remove the non-words like punctuation marks or special\n",
    "characters from the mail documents. There are several ways to do it. Here, we will remove such words after creating a dictionary, which is a very\n",
    "convenient method to do so since when you have a dictionary; you need\n",
    "to remove every such word only once."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "\n",
    "\n",
    "def make_Dictionary(train_dir):\n",
    "    emails = [os.path.join(train_dir,f) for f in os.listdir(train_dir)]    \n",
    "    all_words = []       \n",
    "    for mail in emails:    \n",
    "        with open(mail) as m:\n",
    "            for i,line in enumerate(m):\n",
    "                if i == 2:\n",
    "                    words = line.split()\n",
    "                    all_words += words\n",
    "    \n",
    "    dictionary = Counter(all_words)\n",
    "    \n",
    "    list_to_remove = list(dictionary)\n",
    "    for item in list_to_remove:\n",
    "        if item.isalpha() == False: \n",
    "            del dictionary[item]\n",
    "        elif len(item) == 1:\n",
    "            del dictionary[item]\n",
    "    dictionary = dictionary.most_common(3000)\n",
    "    return dictionary\n",
    "    \n",
    "def extract_features(mail_dir): \n",
    "    files = [os.path.join(mail_dir,fi) for fi in os.listdir(mail_dir)]\n",
    "    features_matrix = np.zeros((len(files),3000))\n",
    "    docID = 0\n",
    "    for fil in files:\n",
    "      with open(fil) as fi:\n",
    "        for i,line in enumerate(fi):\n",
    "          if i == 2:\n",
    "            words = line.split()\n",
    "            for word in words:\n",
    "              wordID = 0\n",
    "              for i,d in enumerate(dictionary):\n",
    "                if d[0] == word:\n",
    "                  wordID = i\n",
    "                  features_matrix[docID,wordID] = words.count(word)\n",
    "        docID = docID + 1     \n",
    "    return features_matrix\n",
    "    \n",
    "# Create a dictionary of words with its frequency\n",
    "train_dir = 'ling-spam\\\\train-mails'\n",
    "dictionary = make_Dictionary(train_dir)\n",
    "\n",
    "# Prepare feature vectors per training mail and its labels\n",
    "train_labels = np.zeros(702)\n",
    "train_labels[351:701] = 1\n",
    "train_matrix = extract_features(train_dir)\n",
    "\n",
    "# Prepare feature vectors per test mail and its labels\n",
    "test_dir = 'ling-spam\\\\test-mails'\n",
    "test_matrix = extract_features(test_dir)\n",
    "test_labels = np.zeros(260)\n",
    "test_labels[130:260] = 1"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 1,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "P_true = 350/702\n",
    "P_false = 1-P_true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_algorithm(train_matrix):\n",
    "        \n",
    "    # true_features_num =  np.sum((train_matrix[351:701]),axis=0)+1\n",
    "    # true_num = np.sum(train_matrix[351:701])+2\n",
    "\n",
    "    # false_features_num =  np.sum((train_matrix[0:350]),axis=0)+1\n",
    "    # false_num = np.sum(train_matrix[0:350])+2\n",
    "\n",
    "    # p_feature_true = np.log(true_features_num/true_num)\n",
    "    # p_feature_false = np.log(false_features_num/false_num)\n",
    "        \n",
    "\n",
    "\n",
    "    p0Num = np.ones(3000)\n",
    "    p1Num = np.ones(3000)\n",
    "    p0Denom = 2.0\n",
    "    p1Denom = 2.0\n",
    "\n",
    "    for i in range(350):\n",
    "        p0Num += train_matrix[i]\n",
    "        p0Denom += sum(train_matrix[i])\n",
    "\n",
    "    for i in range(351,702):\n",
    "        p1Num += train_matrix[i]\n",
    "        p1Denom += sum(train_matrix[i])\n",
    "\n",
    "    p_feature_true = np.log(p1Num / p1Denom)\n",
    "    p_feature_false = np.log(p0Num / p0Denom)\n",
    "\n",
    "    return p_feature_true,p_feature_false"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "p0Num = np.ones(3000)\n",
    "p1Num = np.ones(3000)\n",
    "p0Denom = 2.0\n",
    "p1Denom = 2.0\n",
    "\n",
    "\n",
    "for i in range(350):\n",
    "    p0Num += train_matrix[i]\n",
    "    p0Denom += sum(train_matrix[i])\n",
    "\n",
    "for i in range(351,702):\n",
    "    p1Num += train_matrix[i]\n",
    "    p1Denom += sum(train_matrix[i])\n",
    "\n",
    "\n",
    "p_feature_true = np.log(p1Num / p1Denom)\n",
    "p_feature_false = np.log(p0Num / p0Denom)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "array([-8.60262595, -8.89284607, -4.40386342, ..., -9.5278274 ,\n       -9.39429601, -9.5278274 ])"
     },
     "metadata": {},
     "execution_count": 42
    }
   ],
   "source": [
    "x1 = np.array([[1,2],[3,4])\n",
    "x2 = np.array([3,4])\n",
    "\n",
    "x1 * x2\n",
    "x3 = test_matrix[1,:]+1\n",
    "x3 * p_feature_true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "array([[ 6, 12, 32],\n       [12, 20, 48]])"
     },
     "metadata": {},
     "execution_count": 56
    }
   ],
   "source": [
    "x1 = np.array([[1,2,3],[3,4,5]])\n",
    "x2 = np.array([3,4,8])\n",
    "(x1+1) * x2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "0.9615384615384616"
     },
     "metadata": {},
     "execution_count": 78
    }
   ],
   "source": [
    "def train_algorithm(train_matrix):\n",
    "        \n",
    "    # true_features_num =  np.sum((train_matrix[351:701]),axis=0)+1\n",
    "    # true_num = np.sum(train_matrix[351:701])+2\n",
    "\n",
    "    # false_features_num =  np.sum((train_matrix[0:350]),axis=0)+1\n",
    "    # false_num = np.sum(train_matrix[0:350])+2\n",
    "\n",
    "    # p_feature_true = np.log(true_features_num/true_num)\n",
    "    # p_feature_false = np.log(false_features_num/false_num)\n",
    "        \n",
    "\n",
    "\n",
    "    p0Num = np.ones(3000)\n",
    "    p1Num = np.ones(3000)\n",
    "    p0Denom = 2.0\n",
    "    p1Denom = 2.0\n",
    "\n",
    "    for i in range(350):\n",
    "        p0Num += train_matrix[i]\n",
    "        p0Denom += sum(train_matrix[i])\n",
    "\n",
    "    for i in range(351,702):\n",
    "        p1Num += train_matrix[i]\n",
    "        p1Denom += sum(train_matrix[i])\n",
    "\n",
    "    p_feature_true = np.log(p1Num / p1Denom)\n",
    "    p_feature_false = np.log(p0Num / p0Denom)\n",
    "\n",
    "    return p_feature_true,p_feature_false\n",
    "\n",
    "\n",
    "def pred(test_matrix,p_feature_true,p_feature_false):\n",
    "\n",
    "    p_1 = []\n",
    "    p_0 = []\n",
    "    result = []\n",
    "    test_matrix2 = np.copy(test_matrix)\n",
    "    #test_matrix2[test_matrix2 == 0] = 1\n",
    "    for i in range(260):\n",
    "        p_1.append(np.sum(test_matrix2[i] * p_feature_true) + np.log(P_true))\n",
    "        p_0.append(np.sum(test_matrix2[i] * p_feature_false) + np.log(P_false))\n",
    "   # print(p_1)\n",
    "    #print(p_0)\n",
    "    # p_1 = np.sum((test_matrix+1) * p_feature_true, axis = 1) + np.log(P_true)\n",
    "    # p_0 = np.sum((test_matrix+1) * p_feature_true, axis = 1) + np.log(P_false)\n",
    "\n",
    "    for i in range(260):\n",
    "      #  print(\"p1\" + str(p_1[i]))\n",
    "        \n",
    "       # print(\"p0\" + str(p_0[i])+\"\\n\")\n",
    "        if p_1[i] > p_0[i]:\n",
    "            result.append(1)\n",
    "        else:\n",
    "            result.append(0)\n",
    "\n",
    "    return result\n",
    "\n",
    "    #print(prb_false)\n",
    "\n",
    "p_feature_true,p_feature_false = train_algorithm(train_matrix)\n",
    "\n",
    "result = pred(test_matrix,p_feature_true,p_feature_false)\n",
    "\n",
    "correct = 0\n",
    "for i,item in enumerate(result):\n",
    "    if result[i]==test_labels[i]:\n",
    "        correct+=1\n",
    "correct\n",
    "acc = correct/260\n",
    "acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "138"
     },
     "metadata": {},
     "execution_count": 74
    }
   ],
   "source": [
    "list(result).count(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "0.5076923076923077"
     },
     "metadata": {},
     "execution_count": 65
    }
   ],
   "source": [
    "import math\n",
    "def pred(test_matrix,p_feature_true,p_feature_false):\n",
    "\n",
    "    p_1 = []\n",
    "    p_0 = []\n",
    "    result = []\n",
    "    test_matrix2 = np.copy(test_matrix)\n",
    "    #test_matrix2[test_matrix2 == 0] = 1\n",
    "    for i in range(260):\n",
    "\n",
    "        p_1.append(np.sum((test_matrix2[i,:]+1) * p_feature_true) + np.log(P_true))\n",
    "        p_0.append(np.sum((test_matrix2[i,:]+1) * p_feature_false) + np.log(P_false))\n",
    "\n",
    "    # p_1 = np.sum((test_matrix+1) * p_feature_true, axis = 1) + np.log(P_true)\n",
    "    # p_0 = np.sum((test_matrix+1) * p_feature_true, axis = 1) + np.log(P_false)\n",
    "\n",
    "    for i in range(260):\n",
    "        if p_1[i] > p_0[i]:\n",
    "            result.append(1)\n",
    "        else:\n",
    "            result.append(0)\n",
    "\n",
    "    return result\n",
    "\n",
    "    #print(prb_false)\n",
    "\n",
    "p_feature_true,p_feature_false = train_algorithm(train_matrix)\n",
    "\n",
    "result = pred(test_matrix,p_feature_true,p_feature_false)\n",
    "\n",
    "correct = 0\n",
    "for i,item in enumerate(result):\n",
    "    if result[i]==test_labels[i]:\n",
    "        correct+=1\n",
    "correct\n",
    "acc = correct/260\n",
    "acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "array([ -6.21434021,  -5.8088751 ,  -7.19516946, ...,  -9.83422679,\n       -10.2396919 ,  -9.83422679])"
     },
     "metadata": {},
     "execution_count": 68
    }
   ],
   "source": [
    "p_feature_true\n",
    "p_feature_false"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "Python 3.7.7 64-bit",
   "display_name": "Python 3.7.7 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "0600588c3b5f4418cbe7b5ebc6825b479f3bc010269d8b60d75058cdd010adfe"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}